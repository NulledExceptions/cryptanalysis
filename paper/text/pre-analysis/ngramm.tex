\subsection{Использование n-грамм}

Второй метод анализа так же основан на работе со словарем, 
но обладает рядом преимуществ.

\DEF\textit{n-грамма} — последовательность из n элементов. С 
семантической точки зрения, это может быть последовательность 
звуков, слогов, слов или букв. На практике чаще встречается 
n-грамма как ряд слов. Последовательность из двух последовательных 
элементов часто называют биграммы, последовательность из трех 
элементов называется триграмма. Не менее четырех и выше элементов 
обозначаются как n-грамма, n заменяется на количество последовательных 
элементов.

В области обработки естественного языка, n-граммы используется 
в основном для предугадывания на основе вероятностных моделей. 
n-граммная модель рассчитывает вероятность последнего слова 
n-граммы, если известны все предыдущие. При использовании этого 
подхода для моделирования языка предполагается, что появление 
каждого слова зависит только от предыдущих слов.

\DEF\textit{Инвертированный индекс} — структура данных, в которой 
для каждого слова коллекции документов в соответствующем списке 
перечислены все места в коллекции, в которых оно встретилось. 
Инвертированный индекс используется для поиска по текстам.

Опишем как решается задача нахождения документов в которых встречаются 
все слова из поискового запроса. При обработке однословного поискового 
запроса, ответ уже есть в инвертированном индексе — достаточно 
взять список соответствующий слову из запроса. При обработке 
многословного запроса берутся списки, соответствующие каждому 
из слов запроса и пересекаются.

Пусть у нас есть корпус из трех текстов $T_0$='it is what it is', 
$T_1$='what is it' и $T_2$='it is a banana', тогда инвертированный 
индекс будет выглядеть следующим образом:

\begin{verbatim}
"a":      {2}
"banana": {2}
"is":     {0, 1, 2}
"it":     {0, 1, 2}
"what":   {0, 1}
\end{verbatim}

Здесь цифры обозначают номера текстов, в которых встретилось 
соответствующее слово. Тогда отработка поискового 'what is it
' запроса даст следующий результат $\{0,1\} \cap \{0,1,2\} \cap 
\{0,1,2\} = \{0,1\}$.

\begin{listing}[1]{1}
In [1]: import linguistics
In [2]: a = open('../sample/warandpeace', 'r').readlines()
In [3]: linguistics.istext\_wgramms(a)
0,892340923846
\end{listing}

Время исполнения теста так-же в разы превышает результат 
перебора по словарю:

\begin{listing}[1]{1}
# /usr/bin/time python ./cipher/linguistics.py -w ./sample/warandpeace 
python  210.10s user 280.55s system 0% cpu 20.002 total
\end{listing}

Такой метод, как и словарный перебор не справляется с 
текстом, в котором отсутствуют пробелы. Для решения 
этой проблемы необходимо сделать атомарным элементом 
не слова в тексте, а букву. 

В фреймворке реализован метод поочередного теста текста с 
тетраграммами, триграммами и биграммами. Как только какой-либо 
из тестов получает ответ с велечиной $V$ выше некоторого порога 
(например в 0,8), этот результат возвращается в качестве 
ответа. Такой порядок тестов обусловлен характеристиками 
реализованного лексера. Вызов выглядит так-же как и 
в прошлом примере: 

\begin{listing}[1]{1}
In [1]: import linguistics
In [2]: a = open('../sample/warandpeace', 'r').readlines()
In [3]: linguistics.istext\_lgramms(a)
0,802304958733
\end{listing}

Время исполнения теста:

\begin{listing}[1]{1}
# /usr/bin/time python ./cipher/linguistics.py -l ./sample/warandpeace 
python  170.10s user 170.55s system 0% cpu 20.002 total
\end{listing}

Итак, метод буквенных n-грамм является наиболее эффективным
из рассмотренных и будет использоваться в фреймворке.
