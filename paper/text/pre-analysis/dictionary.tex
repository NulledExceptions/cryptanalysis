\subsection{Словарный перебор}

Наиболее простой метод --- сравнение всех слов текста со словарем 
корректных слов нужного языка. Полученное количество совпавших слов
делится на количество слов исследуемого текста. Результирующая 
величина может рассматриваться как вероятность того, что данный 
текст принадлежит рассматриваемому языку.

Подобная оценка подходит целям данной работы --- критерий по 
этой 'вероятности' позволит отделить зерна от плевел и выделить 
наиболее пригодный вариант --- ложное срабатывание в общем случае 
маловероятно из-за специфики процесса. При негативном результате мы видим 
несвязный набор символов.

\begin{listing}[1]{1}
In [1]: import linguistics
In [2]: a = open('../sample/warandpeace', 'r').readlines()
In [3]: linguistics.istext_dict(a)
('ru', 0,864536523576)
\end{listing}

Здесь в первой строке подключается лингвистический модуль, 
во второй строке тестируемый текст записывается в переменную
$a$, и наконец в третьей вызывается метод определения языка 
по словарю. Метод возвращает предполагаемый язык и 
величину соответствия $V = \frac{n}{N}$, где $n$ --- количество
совпавших слов, а $N$ --- количество слов в тексте.

Практика демонстрирует жизнеспособность такого метода. Во-первых, 
составление 
словаря для известного языка в необходимой стилистике не представляет 
трудности при условии доступности интернета. Во-вторых, современные 
компьютерные мощности позволяют сравнительно быстрое выполнение 
подобного анализа:

\begin{listing}[1]{1}
# /usr/bin/time python ./cipher/linguistics.py -d ./sample/warandpeace 
python  710.10s user 780.55s system 0% cpu 20.002 total
\end{listing}

Здесь мы вызываем метод $istext\_dict$ через внешний 
интерфейс и отслеживаем время выполнения команды с помощью 
стандартной утилиты UNIX time.

Возможно совершествование данного метода путем написания более 
эффективных структур для хранения словаря, грамотной сериализации и 
использования оптимизированных алгоримов поиска по сортированному 
массиву. Но, как будет показано, в этом нет необходимости.
Во-первых мы не перешагнем известное ограничение сложности в 
$O(log(n))$ для алгоритмов поиска. Во-вторых, отсустствует необходимость 
в строгом соответствии текста языку определенному в словаре.
В-третьих, текст шифрограммы сравнительно редко содержит
пробелы, либо им нельзя доверять.

Но, эти пробелы возможно восстановить зная статистическое распределение
слов языка.
То есть необходимо на огромном объеме текста расчитать встречаемость
каждого слова и каждого словосочетания.
Такие данные сложно расчитать, но они уже были получены
в исследовании \cite{google-ngrams}.

Для примера, мы получили фразу \texttt{HELLOHOWAREYOUTODAY} и пытаемся 
определить наиболее вероятное разделение. Возможные разделения 
включают в себя:

\begin{verbatim}
HELLOHOWA REYOUTODAY
HELLO HOW ARE YOU TODAY
HE LL OH OW AR EY OU TO DA Y
H ELLOH OW AREYOU TOD AY
HELL OHO WARE YOU TODAY
\end{verbatim}

и многие другие.

\begin{listing}[1]{1}
In [1]: import cipher.linguistics as linguistics
In [2]: linguistics.restore_words('HELLOHOWAREYOUTODAY')
Out[2]: ['HELLO', 'HOW', 'ARE', 'YOU', 'TODAY']
\end{listing}
